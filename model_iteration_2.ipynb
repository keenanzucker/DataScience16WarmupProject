{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Iteration for Kaggle Titanic Dataset\n",
    "#### This is the improved model. I started by copying all of the work I did for the first iteration, that is, following the tutorial and then working to improve it. The newest model is further below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing all of the libraries that I need! I then read in the training dataset and see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn import *\n",
    "\n",
    "# We can use the pandas library in python to read in the csv file.\n",
    "# This creates a pandas dataframe and assigns it to the titanic variable.\n",
    "titanic = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "# Print the first 5 rows of the dataframe.\n",
    "print(titanic.head(5))\n",
    "print titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all ages were filled in, so I instead filled them in with the median value of all ages. I then changed all of the values of sex from 'male' or 'female' to numbers that I could acutally use in my calculations, being 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I also made the three different values of the port from which was embarked into tangible numbers as well, from S C or Q to 0 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == 'Q', \"Embarked\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used the LinearRegression function from sklearn on the training data, which I split into 3 different arrays, so I could test it against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  8.99877810e-02,   9.60756206e-01,   5.92676278e-01,\n",
      "         9.31138728e-01,   5.29343071e-02,   1.70275685e-01,\n",
      "         3.69943590e-01,   1.03474847e-01,   5.21597906e-01,\n",
      "         8.74491050e-01,   6.48883611e-01,   8.29742769e-01,\n",
      "         1.34797198e-01,  -1.61126844e-01,   6.58141307e-01,\n",
      "         6.39819748e-01,   1.51733875e-01,   2.95432718e-01,\n",
      "         5.35377959e-01,   6.21007683e-01,   2.61872592e-01,\n",
      "         2.62687561e-01,   7.31739160e-01,   5.05995897e-01,\n",
      "         5.61398567e-01,   3.35039734e-01,   1.30338808e-01,\n",
      "         4.68765767e-01,   6.60737753e-01,   9.10819218e-02,\n",
      "         4.77223920e-01,   1.04220026e+00,   6.60691613e-01,\n",
      "         8.71539273e-02,   5.28550732e-01,   4.01874338e-01,\n",
      "         1.30340307e-01,   1.29339672e-01,   5.72717129e-01,\n",
      "         6.65238822e-01,   4.83215779e-01,   7.60807408e-01,\n",
      "         1.30578363e-01,   8.71867121e-01,   7.09855487e-01,\n",
      "         9.11369897e-02,   1.39181745e-01,   6.60691613e-01,\n",
      "         6.82833485e-02,   6.06254374e-01,   4.92254383e-02,\n",
      "         1.29250392e-01,   9.02668258e-01,   7.51677954e-01,\n",
      "         3.19636822e-01,   5.05995897e-01,   8.23411477e-01,\n",
      "         1.27611544e-01,   8.16516947e-01,  -3.70209060e-02,\n",
      "         1.63085464e-01,   9.57981340e-01,   3.96742103e-01,\n",
      "         6.16138409e-02,   5.42714233e-01,   6.62112275e-02,\n",
      "         7.79751268e-01,   1.40293401e-01,   4.40592742e-01,\n",
      "         3.50534388e-02,   2.72709814e-01,   4.26360339e-01,\n",
      "         3.55241143e-01,   1.10226880e-01,   8.66078358e-02,\n",
      "         1.07366720e-01,   9.10819218e-02,   9.11369897e-02,\n",
      "         3.82661024e-01,   5.72471068e-01,   1.24221410e-01,\n",
      "         8.61972872e-02,   6.60705005e-01,   5.10138486e-01,\n",
      "         8.45241581e-01,   4.56477760e-01,   3.22699204e-02,\n",
      "         9.11369897e-02,   9.37604538e-01,   1.12967094e-01,\n",
      "         8.56794636e-02,   1.34727274e-01,   3.83320807e-01,\n",
      "         6.14970393e-03,  -7.83320148e-02,   9.11369897e-02,\n",
      "         3.10516665e-01,   5.49345421e-01,   7.23544338e-01,\n",
      "         2.33721448e-01,   5.81750798e-01,   9.10819218e-02,\n",
      "         5.25738424e-01,   6.40651310e-02,  -2.52427240e-02,\n",
      "         9.10819218e-02,   6.19865700e-01,   9.10387818e-02,\n",
      "         3.65066610e-02,   6.32939707e-01,   4.08195377e-01,\n",
      "         6.63657306e-01,   1.23882146e-01,   5.92491292e-01,\n",
      "         6.83623624e-01,   1.29295032e-01,  -6.19221217e-02,\n",
      "         2.59223480e-01,   6.09655955e-01,   5.30794378e-01,\n",
      "         2.88023805e-01,   9.11369897e-02,   2.82857942e-01,\n",
      "         7.61542726e-01,   3.45640063e-01,   1.85484998e-01,\n",
      "         1.70022737e-01,   1.12642722e-01,   5.59420117e-01,\n",
      "        -2.02485747e-03,   1.03290733e-01,   1.34440079e-01,\n",
      "         4.46807623e-01,   7.51677954e-01,   3.11805296e-01,\n",
      "         3.62947385e-01,   9.75724449e-01,   4.29554800e-01,\n",
      "         1.57043954e-01,   5.82928575e-01,   5.57105476e-01,\n",
      "         6.14443886e-01,   5.72812834e-01,   2.18783352e-01,\n",
      "         3.49472299e-01,   2.86040080e-01,   9.65037360e-02,\n",
      "         5.60916106e-01,   1.86919710e-01,   2.19027353e-01,\n",
      "         1.69739986e-01,   1.00690768e+00,  -5.89449777e-02,\n",
      "        -4.15452572e-02,   9.08736139e-02,   3.95827915e-01,\n",
      "         7.26175962e-01,   8.02219375e-02,   9.13557255e-02,\n",
      "        -2.22536096e-01,  -2.66919104e-02,   7.21593360e-01,\n",
      "         1.01953834e-01,   1.51388512e-01,   8.19705948e-02,\n",
      "         1.32518461e-01,   9.70245311e-01,   3.28974893e-01,\n",
      "         5.02576476e-01,   1.08437940e-01,   3.25183297e-01,\n",
      "         1.40818823e-01,   6.63268211e-01,   1.29295032e-01,\n",
      "         3.90965934e-01,   7.86503606e-02,  -3.68524682e-02,\n",
      "         9.13671691e-01,   2.84517666e-01,   4.46019673e-02,\n",
      "         2.68132779e-01,   3.35661255e-01,   1.96299597e-03,\n",
      "         3.51470400e-01,   6.51010647e-01,   5.11174133e-01,\n",
      "         6.29850621e-01,   4.10021732e-01,   4.03081359e-02,\n",
      "         4.74217131e-02,   7.64271489e-01,   3.44550453e-01,\n",
      "         5.97245007e-01,   3.69521460e-01,   9.46062691e-01,\n",
      "         9.12083149e-01,   1.70022737e-01,  -1.85251802e-02,\n",
      "         6.60691613e-01,   8.07931698e-01,   9.16548133e-02,\n",
      "        -2.22536096e-01,   5.78367977e-02,   3.48321010e-02,\n",
      "         1.45712251e-01,   6.91179799e-01,   3.84837497e-02,\n",
      "         1.45383056e-01,   7.26181926e-01,   4.78394987e-01,\n",
      "         1.12609974e-01,   7.50755869e-01,   1.23596450e-01,\n",
      "         2.84517666e-01,   1.36414068e-01,   1.01395495e+00,\n",
      "         5.87218752e-01,   1.90418359e-01,   1.02889863e+00,\n",
      "         2.83624866e-01,   1.56627303e-01,   3.00890244e-01,\n",
      "        -3.43861103e-02,   9.10819218e-02,   4.37274991e-01,\n",
      "         1.24346402e-01,   3.43657653e-01,   1.31782740e-01,\n",
      "         3.50007979e-01,   4.53816408e-01,   9.41986239e-01,\n",
      "         8.55812557e-02,   1.26427969e-01,   5.14461976e-01,\n",
      "         3.16370023e-01,   5.81627306e-01,   1.79146187e-01,\n",
      "         8.33217359e-01,   3.43657653e-01,   2.67886176e-01,\n",
      "         5.89980704e-01,   6.29850621e-01,   2.89082393e-01,\n",
      "         1.23551810e-01,   1.19423755e-01,   4.49914049e-01,\n",
      "         5.98080236e-01,   7.41700785e-01,   3.95976588e-01,\n",
      "         1.24570927e-01,   9.08512939e-02,   5.10217925e-01,\n",
      "         3.17243789e-01,   4.94880818e-02,   4.48434902e-01,\n",
      "         5.51647950e-01,   1.05176735e+00,   1.00396283e+00,\n",
      "         1.16824364e+00,   6.37295280e-01,   1.70022737e-01,\n",
      "         3.47081525e-02,   3.23790141e-01,   4.27827834e-01,\n",
      "         6.60691613e-01,   2.50879710e-01,   1.07703504e-04,\n",
      "         7.38026906e-02,   8.41682429e-01,   9.94221666e-01,\n",
      "         5.04388858e-01,   1.04634754e-01,   6.84091736e-01,\n",
      "         4.60920013e-01,   6.60691613e-01,   7.87205387e-01,\n",
      "         4.88920786e-01,   2.90790162e-01,   1.24446245e-01,\n",
      "         4.80968077e-01,  -3.19057282e-02,   9.10670657e-02,\n",
      "         1.57145126e-01,   1.40254724e-01,   5.02603260e-01,\n",
      "         1.03564537e-01,   8.07397611e-02,   1.23827078e-01,\n",
      "         2.19027353e-01,   6.93436769e-01,   1.02306096e+00,\n",
      "         1.07151871e+00,   2.91224311e-01,   6.03921666e-01,\n",
      "         1.12912026e-01,   5.42714233e-01,   1.54899175e-01]), array([ 1.13774791,  0.44173212,  0.98551347,  0.66915371,  0.08254228,\n",
      "        0.15142624,  0.83642014,  0.09704526,  0.64711481,  1.03845173,\n",
      "        1.06064212,  0.24647842,  0.98364902,  1.04411609,  1.10195734,\n",
      "        0.72596387,  0.09692709,  0.11388411,  0.60824987,  0.74905725,\n",
      "        0.090424  ,  1.00314273,  0.91588368,  0.13679886,  0.10365487,\n",
      "        0.82296458,  0.755174  , -0.27746285,  1.0035964 , -0.12636043,\n",
      "        0.70865678,  0.52438799,  1.06900476,  0.58044138,  0.32246331,\n",
      "        0.45904751,  0.0848131 ,  0.96838383,  0.09692709,  0.4123739 ,\n",
      "        0.96908901, -0.01732698,  0.33119158,  0.38953146,  0.97455471,\n",
      "        0.26457991,  0.28476325,  0.21075768,  0.78939013,  0.68174567,\n",
      "        0.5508181 ,  0.21132238,  0.00332574,  0.1315846 ,  0.44518065,\n",
      "        0.16116388,  0.07440511,  0.13363265,  0.09815645,  0.98913539,\n",
      "        0.69520122,  0.66925272,  0.66925272, -0.05732283,  0.25605759,\n",
      "        0.51306171,  0.04918447,  0.12689844,  0.08297663,  0.74556032,\n",
      "        0.63153497,  0.66915371,  1.03349593,  0.46795359,  0.11283671,\n",
      "        0.15759527,  0.5998862 ,  0.6125967 ,  0.96615292,  0.63469796,\n",
      "        0.6051113 ,  0.18499302,  0.15738453,  1.03364995,  0.80043282,\n",
      "        0.07003835,  0.85871777,  0.09692709,  0.37822123,  0.03771546,\n",
      "        0.70865678,  0.17123866,  0.87293786,  0.38692632,  0.14394491,\n",
      "       -0.00364112,  1.02362819,  0.60920867,  0.13721713,  0.57461098,\n",
      "        0.1534423 ,  0.29630296,  0.76221079,  0.0229439 ,  0.11050082,\n",
      "        0.59310377,  0.05272741,  0.64923598,  0.18004866, -0.05792355,\n",
      "        0.37724772,  0.14392897,  0.44776777,  0.09692709,  0.17057126,\n",
      "        0.97573347,  0.2546175 , -0.01069499,  0.59494436,  0.67712284,\n",
      "        0.81048116,  0.25112435,  0.7091068 ,  0.13414671,  0.21833626,\n",
      "        0.09018337,  0.5398775 ,  0.11371054,  0.09643219,  0.72214613,\n",
      "        0.83299143,  0.1712546 ,  0.07013414,  0.43870508,  0.5508181 ,\n",
      "        0.62795723,  0.17034196,  0.26289071,  1.03283656,  0.54234647,\n",
      "        0.66429253,  0.2888594 ,  0.24248073,  0.59832765,  0.15197868,\n",
      "        0.06672256,  0.76247901,  0.09709316,  0.62328105,  0.85873908,\n",
      "        0.39833841,  0.68526385,  0.28026543,  0.15249025,  0.0558822 ,\n",
      "        0.46338875,  0.3322838 ,  0.09704526,  0.12741893,  0.18977726,\n",
      "        0.90570685,  0.61255203,  0.1712546 ,  0.3041495 ,  0.05667859,\n",
      "        0.32003504,  0.13002433,  0.09704526,  0.02900113,  0.2546175 ,\n",
      "        0.25032727,  0.17123545,  0.71385691,  0.09643219,  0.03023685,\n",
      "        0.67057269,  0.83394424,  0.63668087,  0.45820842,  0.18004866,\n",
      "        0.03925263,  0.13700639,  0.76347615, -0.01610677,  0.2546175 ,\n",
      "       -0.05096587,  0.36065035,  0.49526401,  0.44776777,  0.88783867,\n",
      "        0.27650531,  0.0835897 ,  0.17095571,  0.0558822 ,  0.14352664,\n",
      "        0.26008209,  0.20422092,  0.14413971,  0.13917582,  0.78823881,\n",
      "        0.10244795,  0.983009  ,  0.12376157,  0.17152021,  0.71624816,\n",
      "        0.66906113,  0.5355726 ,  1.06327957,  0.55601524,  0.71952689,\n",
      "        0.43870508,  0.10813802,  0.14762674,  0.16452683,  0.09704526,\n",
      "        0.38468169,  0.77378051,  0.12353167,  0.31660245,  0.72019649,\n",
      "        0.18382257,  0.6683239 ,  0.07001598,  0.97445504,  0.13729376,\n",
      "        0.13363265,  0.88062695,  0.13363587,  0.08715737,  0.61255203,\n",
      "        0.5883169 ,  0.0229439 ,  0.18684089,  0.88743056,  0.13363587,\n",
      "        0.14770832,  0.62385335,  0.58195819,  0.89464072,  0.32433284,\n",
      "        1.0215796 ,  0.10198815,  1.01250232,  0.89757009,  0.52011358,\n",
      "        0.50665802,  0.19733591,  0.33882963,  0.19608356,  0.78269614,\n",
      "        0.3024605 ,  0.01303333,  0.35740293,  0.59528255,  0.2812701 ,\n",
      "        0.1713153 ,  0.17399933,  0.63510029,  0.2099606 ,  0.79897366,\n",
      "        0.62993975,  0.84335812,  0.49799211,  0.1712546 ,  0.01619374,\n",
      "        0.26496308,  0.09704526,  0.59494436,  0.03570385,  0.1574771 ,\n",
      "        0.55964686,  0.13363587,  0.0699841 ,  0.03391958,  0.68692335,\n",
      "        0.38475832,  0.66915371,  0.17777861,  0.16253816,  0.72211234,\n",
      "        0.83479538,  0.58677963,  0.07003835,  0.735757  ,  0.90451305,\n",
      "        0.09962007,  0.43250553,  0.13477258,  1.02529894,  0.13828479,\n",
      "        0.24105043,  0.13741193,  0.09704526,  0.04924194,  0.80169436,\n",
      "       -0.03139561,  0.64987806]), array([  1.72889219e-01,   1.70294715e-02,   7.82616935e-01,\n",
      "        -8.34788848e-03,   1.47022266e-01,   3.10888595e-01,\n",
      "         7.28261340e-01,   1.01479914e-01,   4.24565622e-01,\n",
      "         1.57316587e-02,   4.37708069e-01,   1.44204264e-02,\n",
      "         9.07678482e-02,   4.33913871e-01,   8.26537251e-01,\n",
      "         8.45262338e-01,   5.42776171e-01,   1.01763663e-01,\n",
      "         6.70148479e-01,   1.92163452e-01,   6.39359534e-02,\n",
      "         7.62650655e-01,   3.10124701e-02,   5.90024631e-01,\n",
      "         8.31356231e-01,   2.78648916e-01,   1.08309653e-01,\n",
      "         3.04531238e-01,   1.50864127e-01,   1.38986099e-01,\n",
      "         1.36219795e-01,   2.51197915e-01,   2.02625887e-01,\n",
      "         9.72357134e-01,   1.12191979e-01,   1.92169054e-01,\n",
      "         1.50211875e-01,  -2.14264992e-02,   4.52451020e-01,\n",
      "         4.38789988e-01,   6.04820088e-01,   7.89326541e-01,\n",
      "         8.00459867e-02,   2.10435721e-01,   5.70885269e-01,\n",
      "         5.70841743e-02,   1.44342132e-01,   1.00451104e+00,\n",
      "         6.42312317e-01,   8.51755703e-02,   7.33373007e-01,\n",
      "         3.09602117e-01,   1.49684208e-01,   3.22228832e-01,\n",
      "         1.01595923e-01,   6.50604478e-01,   1.01479914e-01,\n",
      "         8.45026241e-01,   1.38791822e-01,   7.14365273e-01,\n",
      "         7.68287651e-01,   1.84938938e-01,   1.01479914e-01,\n",
      "         6.54218524e-01,   2.93878313e-01,   2.96413137e-01,\n",
      "         1.92833539e-01,   8.27498735e-02,   3.28441263e-01,\n",
      "         5.87658439e-02,   1.02674988e-01,   1.42090676e-01,\n",
      "         2.83166248e-01,   1.01520440e-01,   2.10876914e-02,\n",
      "         9.01930011e-01,   6.80182444e-01,   3.63633521e-01,\n",
      "         4.29834748e-02,   2.51030051e-01,   2.71459394e-01,\n",
      "         1.55080767e-01,   1.20174297e-01,   6.76615822e-01,\n",
      "         5.21604336e-01,   2.74876851e-01,   7.14261845e-01,\n",
      "         4.63722197e-01,   1.43882255e-01,  -3.38493769e-02,\n",
      "         5.08333972e-02,   2.88240761e-01,   4.71949096e-03,\n",
      "         1.48920991e-01,   1.55073789e-01,   9.65241409e-01,\n",
      "         3.61956120e-01,   8.01212426e-01,   8.51755703e-02,\n",
      "         1.63090365e-01,   2.58489938e-01,   1.38385623e-01,\n",
      "         1.57316587e-02,   7.14397446e-01,   2.98282232e-01,\n",
      "         2.65779163e-02,   9.41922468e-01,   3.92478820e-01,\n",
      "         7.25879907e-01,   2.08234335e-01,   7.05625434e-02,\n",
      "         2.03820545e-01,   6.98106244e-01,   3.54986591e-01,\n",
      "         9.42312534e-01,   1.08182230e-01,   1.01115214e+00,\n",
      "         4.29882986e-01,   2.72580965e-01,   9.55913060e-02,\n",
      "         1.38553363e-01,   1.49766670e-01,   8.76445205e-01,\n",
      "         7.95521275e-01,   1.89563479e-01,   7.47402760e-02,\n",
      "         9.05943831e-01,   1.19035222e-01,   2.34961953e-01,\n",
      "         1.49265429e-01,   3.84688624e-01,   1.44070963e-01,\n",
      "         6.51000458e-01,   7.14396037e-01,   2.37161612e-01,\n",
      "         5.98123216e-01,   8.84762775e-01,   2.34195832e-01,\n",
      "         2.71459394e-01,   2.93878313e-01,   2.93878313e-01,\n",
      "         9.60495497e-02,   4.82543535e-01,   2.74738708e-01,\n",
      "         1.01479914e-01,   1.01479914e-01,   4.28725578e-01,\n",
      "         3.27845711e-01,   8.83507841e-01,   7.85083053e-02,\n",
      "         8.54020195e-02,   1.53868294e-01,   1.25458500e-01,\n",
      "         7.78614476e-01,   4.27536886e-01,   1.76095354e-01,\n",
      "         8.78367308e-01,   2.23270579e-01,   7.41615725e-02,\n",
      "         1.28260077e-01,   6.34105869e-01,   3.76826088e-01,\n",
      "         1.01513462e-01,   3.21161697e-01,   6.92919862e-02,\n",
      "         9.05219168e-01,   9.92643346e-02,   3.21100762e-02,\n",
      "         1.89869119e-01,   8.47257439e-01,   1.65792833e-01,\n",
      "         7.70032759e-01,   4.70822280e-01,   7.01001762e-01,\n",
      "         1.45018183e-01,   7.98992141e-02,   1.22365867e-01,\n",
      "        -5.62678525e-03,   6.34840292e-01,   1.47022266e-01,\n",
      "         6.21554022e-01,   1.55089154e-01,   1.92163452e-01,\n",
      "         7.45360827e-01,   1.92167645e-01,   8.15272492e-01,\n",
      "         7.49589740e-01,   9.59168970e-01,   4.23369546e-01,\n",
      "         6.56067455e-02,   1.17831761e-01,   1.17764665e-01,\n",
      "         6.77402825e-01,   1.31033823e-01,   2.11184136e-01,\n",
      "         3.61128670e-01,   1.92163452e-01,   3.27009298e-01,\n",
      "         2.80865752e-01,   4.73809464e-01,   1.17548012e-01,\n",
      "         2.08181789e-01,   8.39842956e-01,   6.07376016e-01,\n",
      "         1.36308792e-01,   5.71394060e-01,   2.34961953e-01,\n",
      "         7.32664113e-01,   4.58929866e-01,   2.99802486e-01,\n",
      "         1.07144857e-01,   8.54523415e-02,   3.79873628e-01,\n",
      "         6.77309159e-01,   2.08181789e-01,   8.74780819e-01,\n",
      "         1.12194764e-01,   3.71105893e-02,   2.30444621e-01,\n",
      "         5.78112549e-01,   8.80381008e-02,   4.38789988e-01,\n",
      "         6.50478673e-01,   2.52145211e-01,   2.16244600e-02,\n",
      "         7.72356638e-02,   7.64956968e-01,   1.06578734e-01,\n",
      "         3.85229660e-01,   6.33022282e-01,   6.89918839e-02,\n",
      "         1.92431836e-01,   8.51755703e-02,   4.59963761e-01,\n",
      "         1.92163452e-01,   7.52074841e-01,   6.94810438e-01,\n",
      "         3.74543331e-01,   1.47020857e-01,   1.28274033e-01,\n",
      "         1.54904640e-01,   8.83372143e-01,   1.38714930e-01,\n",
      "         1.01428183e-01,   6.37514393e-02,   4.74143535e-01,\n",
      "         1.44318380e-01,   3.32209243e-01,   9.85223737e-01,\n",
      "         1.12472244e-01,   1.60139061e-01,   2.66114644e-02,\n",
      "        -2.41362640e-01,   1.09304997e-01,   2.65882719e-01,\n",
      "         9.34799595e-01,   6.65962224e-02,  -1.44857067e-01,\n",
      "         7.32175244e-01,   1.01756702e+00,   6.57625381e-01,\n",
      "         6.82274953e-01,   7.78507074e-01,   3.06694232e-01,\n",
      "         7.03120381e-01,   1.47020857e-01,  -5.35194672e-02,\n",
      "         2.63450207e-01,   8.45198988e-01,   2.80865752e-01,\n",
      "         2.88522280e-01,   7.14342083e-01,   7.98068552e-01,\n",
      "         4.05781543e-01,   1.00941736e-01,   1.92789366e-01,\n",
      "         1.12191979e-01,   8.05473642e-01,   4.10332423e-01,\n",
      "        -6.55145848e-04,   7.89310178e-01,   7.38879084e-01,\n",
      "         1.43673989e-01,   1.49684208e-01,   1.01479914e-01,\n",
      "         8.33962978e-01,   8.06527571e-01,   7.46997500e-02,\n",
      "         6.54965242e-01,   2.67936850e-01,   1.17831761e-01,\n",
      "         6.75775470e-01,   2.72454182e-01,   9.99158265e-01,\n",
      "         5.87835137e-01,   4.84754956e-01,   1.70739321e-01])]\n"
     ]
    }
   ],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "print predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then put the three arrays back into one, and made the predictions binary again, either 0 or 1 for survival, so I could test vs the actual predictions of survivial or not. I counted all the correct ones and divided by the total number to get an accuracy rating of ~78 percent, which isn't great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783389450056\n"
     ]
    }
   ],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "# print predictions\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "# print predictions, titanic[\"Survived\"]\n",
    "correct = 0.0\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "     if (predictions[i] == titanic[\"Survived\"][i]):\n",
    "        correct += 1\n",
    "        \n",
    "accuracy = correct / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a Logistic Regression function from sklearen to compute the accuracy across all of the three different folds that I had used before and averaged the results, giving me a similar accuracy percent of ~78.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = linear_model.LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then cleaned the test data with the same conversions to numeric catagories as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "     PassengerId  Pclass                                               Name  \\\n",
      "0            892       3                                   Kelly, Mr. James   \n",
      "1            893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                          Myles, Mr. Thomas Francis   \n",
      "3            895       3                                   Wirz, Mr. Albert   \n",
      "4            896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "5            897       3                         Svensson, Mr. Johan Cervin   \n",
      "6            898       3                               Connolly, Miss. Kate   \n",
      "7            899       2                       Caldwell, Mr. Albert Francis   \n",
      "8            900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
      "9            901       3                            Davies, Mr. John Samuel   \n",
      "10           902       3                                   Ilieff, Mr. Ylio   \n",
      "11           903       1                         Jones, Mr. Charles Cresson   \n",
      "12           904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
      "13           905       2                               Howard, Mr. Benjamin   \n",
      "14           906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
      "15           907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
      "16           908       2                                  Keane, Mr. Daniel   \n",
      "17           909       3                                  Assaf, Mr. Gerios   \n",
      "18           910       3                       Ilmakangas, Miss. Ida Livija   \n",
      "19           911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
      "20           912       1                             Rothschild, Mr. Martin   \n",
      "21           913       3                          Olsen, Master. Artur Karl   \n",
      "22           914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
      "23           915       1                    Williams, Mr. Richard Norris II   \n",
      "24           916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
      "25           917       3                            Robins, Mr. Alexander A   \n",
      "26           918       1                       Ostby, Miss. Helene Ragnhild   \n",
      "27           919       3                                  Daher, Mr. Shedid   \n",
      "28           920       1                            Brady, Mr. John Bertram   \n",
      "29           921       3                                  Samaan, Mr. Elias   \n",
      "..           ...     ...                                                ...   \n",
      "388         1280       3                               Canavan, Mr. Patrick   \n",
      "389         1281       3                        Palsson, Master. Paul Folke   \n",
      "390         1282       1                         Payne, Mr. Vivian Ponsonby   \n",
      "391         1283       1     Lines, Mrs. Ernest H (Elizabeth Lindsey James)   \n",
      "392         1284       3                      Abbott, Master. Eugene Joseph   \n",
      "393         1285       2                               Gilbert, Mr. William   \n",
      "394         1286       3                           Kink-Heilmann, Mr. Anton   \n",
      "395         1287       1     Smith, Mrs. Lucien Philip (Mary Eloise Hughes)   \n",
      "396         1288       3                               Colbert, Mr. Patrick   \n",
      "397         1289       1  Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...   \n",
      "398         1290       3                     Larsson-Rondberg, Mr. Edvard A   \n",
      "399         1291       3                           Conlon, Mr. Thomas Henry   \n",
      "400         1292       1                            Bonnell, Miss. Caroline   \n",
      "401         1293       2                                    Gale, Mr. Harry   \n",
      "402         1294       1                     Gibson, Miss. Dorothy Winifred   \n",
      "403         1295       1                             Carrau, Mr. Jose Pedro   \n",
      "404         1296       1                       Frauenthal, Mr. Isaac Gerald   \n",
      "405         1297       2       Nourney, Mr. Alfred (Baron von Drachstedt\")\"   \n",
      "406         1298       2                          Ware, Mr. William Jeffery   \n",
      "407         1299       1                         Widener, Mr. George Dunton   \n",
      "408         1300       3                    Riordan, Miss. Johanna Hannah\"\"   \n",
      "409         1301       3                          Peacock, Miss. Treasteall   \n",
      "410         1302       3                             Naughton, Miss. Hannah   \n",
      "411         1303       1    Minahan, Mrs. William Edward (Lillian E Thorpe)   \n",
      "412         1304       3                     Henriksson, Miss. Jenny Lovisa   \n",
      "413         1305       3                                 Spector, Mr. Woolf   \n",
      "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                                Ware, Mr. Frederick   \n",
      "417         1309       3                           Peter, Master. Michael J   \n",
      "\n",
      "    Sex   Age  SibSp  Parch              Ticket      Fare            Cabin  \\\n",
      "0     0  34.5      0      0              330911    7.8292              NaN   \n",
      "1     1  47.0      1      0              363272    7.0000              NaN   \n",
      "2     0  62.0      0      0              240276    9.6875              NaN   \n",
      "3     0  27.0      0      0              315154    8.6625              NaN   \n",
      "4     1  22.0      1      1             3101298   12.2875              NaN   \n",
      "5     0  14.0      0      0                7538    9.2250              NaN   \n",
      "6     1  30.0      0      0              330972    7.6292              NaN   \n",
      "7     0  26.0      1      1              248738   29.0000              NaN   \n",
      "8     1  18.0      0      0                2657    7.2292              NaN   \n",
      "9     0  21.0      2      0           A/4 48871   24.1500              NaN   \n",
      "10    0  28.0      0      0              349220    7.8958              NaN   \n",
      "11    0  46.0      0      0                 694   26.0000              NaN   \n",
      "12    1  23.0      1      0               21228   82.2667              B45   \n",
      "13    0  63.0      1      0               24065   26.0000              NaN   \n",
      "14    1  47.0      1      0         W.E.P. 5734   61.1750              E31   \n",
      "15    1  24.0      1      0       SC/PARIS 2167   27.7208              NaN   \n",
      "16    0  35.0      0      0              233734   12.3500              NaN   \n",
      "17    0  21.0      0      0                2692    7.2250              NaN   \n",
      "18    1  27.0      1      0    STON/O2. 3101270    7.9250              NaN   \n",
      "19    1  45.0      0      0                2696    7.2250              NaN   \n",
      "20    0  55.0      1      0            PC 17603   59.4000              NaN   \n",
      "21    0   9.0      0      1             C 17368    3.1708              NaN   \n",
      "22    1  28.0      0      0            PC 17598   31.6833              NaN   \n",
      "23    0  21.0      0      1            PC 17597   61.3792              NaN   \n",
      "24    1  48.0      1      3            PC 17608  262.3750  B57 B59 B63 B66   \n",
      "25    0  50.0      1      0           A/5. 3337   14.5000              NaN   \n",
      "26    1  22.0      0      1              113509   61.9792              B36   \n",
      "27    0  22.5      0      0                2698    7.2250              NaN   \n",
      "28    0  41.0      0      0              113054   30.5000              A21   \n",
      "29    0  28.0      2      0                2662   21.6792              NaN   \n",
      "..   ..   ...    ...    ...                 ...       ...              ...   \n",
      "388   0  21.0      0      0              364858    7.7500              NaN   \n",
      "389   0   6.0      3      1              349909   21.0750              NaN   \n",
      "390   0  23.0      0      0               12749   93.5000              B24   \n",
      "391   1  51.0      0      1            PC 17592   39.4000              D28   \n",
      "392   0  13.0      0      2           C.A. 2673   20.2500              NaN   \n",
      "393   0  47.0      0      0          C.A. 30769   10.5000              NaN   \n",
      "394   0  29.0      3      1              315153   22.0250              NaN   \n",
      "395   1  18.0      1      0               13695   60.0000              C31   \n",
      "396   0  24.0      0      0              371109    7.2500              NaN   \n",
      "397   1  48.0      1      1               13567   79.2000              B41   \n",
      "398   0  22.0      0      0              347065    7.7750              NaN   \n",
      "399   0  31.0      0      0               21332    7.7333              NaN   \n",
      "400   1  30.0      0      0               36928  164.8667               C7   \n",
      "401   0  38.0      1      0               28664   21.0000              NaN   \n",
      "402   1  22.0      0      1              112378   59.4000              NaN   \n",
      "403   0  17.0      0      0              113059   47.1000              NaN   \n",
      "404   0  43.0      1      0               17765   27.7208              D40   \n",
      "405   0  20.0      0      0       SC/PARIS 2166   13.8625              D38   \n",
      "406   0  23.0      1      0               28666   10.5000              NaN   \n",
      "407   0  50.0      1      1              113503  211.5000              C80   \n",
      "408   1  28.0      0      0              334915    7.7208              NaN   \n",
      "409   1   3.0      1      1  SOTON/O.Q. 3101315   13.7750              NaN   \n",
      "410   1  28.0      0      0              365237    7.7500              NaN   \n",
      "411   1  37.0      1      0               19928   90.0000              C78   \n",
      "412   1  28.0      0      0              347086    7.7750              NaN   \n",
      "413   0  28.0      0      0           A.5. 3236    8.0500              NaN   \n",
      "414   1  39.0      0      0            PC 17758  108.9000             C105   \n",
      "415   0  38.5      0      0  SOTON/O.Q. 3101262    7.2500              NaN   \n",
      "416   0  28.0      0      0              359309    8.0500              NaN   \n",
      "417   0  28.0      1      1                2668   22.3583              NaN   \n",
      "\n",
      "    Embarked  \n",
      "0          2  \n",
      "1          0  \n",
      "2          2  \n",
      "3          0  \n",
      "4          0  \n",
      "5          0  \n",
      "6          2  \n",
      "7          0  \n",
      "8          1  \n",
      "9          0  \n",
      "10         0  \n",
      "11         0  \n",
      "12         0  \n",
      "13         0  \n",
      "14         0  \n",
      "15         1  \n",
      "16         2  \n",
      "17         1  \n",
      "18         0  \n",
      "19         1  \n",
      "20         1  \n",
      "21         0  \n",
      "22         0  \n",
      "23         1  \n",
      "24         1  \n",
      "25         0  \n",
      "26         1  \n",
      "27         1  \n",
      "28         0  \n",
      "29         1  \n",
      "..       ...  \n",
      "388        2  \n",
      "389        0  \n",
      "390        0  \n",
      "391        0  \n",
      "392        0  \n",
      "393        0  \n",
      "394        0  \n",
      "395        0  \n",
      "396        2  \n",
      "397        1  \n",
      "398        0  \n",
      "399        2  \n",
      "400        0  \n",
      "401        0  \n",
      "402        1  \n",
      "403        0  \n",
      "404        1  \n",
      "405        1  \n",
      "406        0  \n",
      "407        1  \n",
      "408        2  \n",
      "409        0  \n",
      "410        2  \n",
      "411        2  \n",
      "412        0  \n",
      "413        0  \n",
      "414        1  \n",
      "415        0  \n",
      "416        0  \n",
      "417        1  \n",
      "\n",
      "[418 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "print(titanic_test[\"Sex\"].unique())\n",
    "\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'male', \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'female', \"Sex\"] = 1\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'Q', \"Embarked\"] = 2\n",
    "\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "\n",
    "print titanic_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then made predictions on the test set using the training algorithm from beforehand. Finally, I created a submission file for kaggle in the form of a csv for submission! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = linear_model.LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "# submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One Complete! Now on to making the model better\n",
    "_____\n",
    "\n",
    "I want to improve my model to get a better accuracy score than ~78 percent. I'll try to do this by implementing a random forest classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.801346801347\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "print\"Score:\" , scores.mean();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also try some parameter tuning to improve my score as well. Now I am using 150 estimators, which will take longer but hopefully improve the mean score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.820426487093\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I try even more samples, I will get a slightly better score again. I'll jump up to 300 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.821548821549\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also increase the number of folds in the data, which should increase the score as well, although only slightly. This creates more evening out since it is comparing more samples to one another. I changed the number of folds from 3 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.833931853718\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This increased my score in theory, so now I want to see how it compares to the rest of the dataset! I'm going to create another submission for kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "# submission.to_csv(\"kaggle2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model 2 --> Improving the Model Even More\n",
    "\n",
    "#### Now I will experiment more, using insiration from other user to make a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with looking at age. Currently, the age is being evaluated on a linear scale. This however, might make more sense on a log scale, as a difference in age when younger could make more of a difference than when older. To test this, I'm going to use a log10 on the age catagory, and try the same algorithms again, to see if we can improve that score. I got this inspiration from in class, where we discussed how logging age would improve the score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.835042824157\n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = np.log10(titanic[\"Age\"])\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this did improve the score, although not by much. My score, in theory, is now up to ~83.5, which is slowly getting better! Let's try another strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# titanic[\"Age\"] = np.log2(titanic[\"Age\"])\n",
    "# predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "# alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "# scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "# print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tried again with log base 2 instead of the log base 10 parameter. I've commented out the commands because running both this one and the log10 will return a long error. But running the log2 instead of log10 actually did not change the score, still returning a ~83.5! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to look into the sibling and parent child parameters. These seem very related, but also not as important as some of the other features. So, I'm going to combine them into one catagory, I'll call 'FamilyTies.' Hopefully, having one less catagory will make the other's more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.826072891321\n"
     ]
    }
   ],
   "source": [
    "# Generating a family column, and then dropping the other two columns\n",
    "titanic[\"FamilyTies\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "titanic.drop('SibSp', axis=1, inplace=True)\n",
    "titanic.drop('Parch', axis=1, inplace=True)\n",
    "\n",
    "# Now we will try to generate another algorithm, still using the random forest, to see if this improved my score!\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyTies\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\" , scores.mean()\n",
    "\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# # Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "# submission = pandas.DataFrame({\n",
    "#         \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "#         \"Survived\": predictions\n",
    "#     })\n",
    "    \n",
    "# submission.to_csv(\"kaggle5.csv\", index=False)\n",
    "\n",
    "# This raised my overall score by about a half percent on kaggle! Yay!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, this actually dropped my score down to a ~83.4, which isn't a whole lot, but does make a difference. However, this is still working only with the dataset I have, and not the one that Kaggle uses to test the actual score, so it may not be necessarily that much worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "Another feature that I saw used in the dataquest of improving the submission was an example that used the titles of the peoples names to try to detemine whether certain titles would improve someone's survival chances. I thought this was interesting, so I decided to try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.832833437419\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "\n",
    "#Now we will run the algorithm again, using the new column of Title!\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyTies\", \"Title\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\" , scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the score went down again! Bummer! Again, because the change wasn't particularily large, it might not be actually a worse model, but instead just the training data vs the ones that kaggle uses to test. But I won't really know until I submit. But I want to try some other things first! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.83950617284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keenan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:42: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "\n",
    "# Originally 25 estimators at max depth of 3 --> Editted to get better accuracy\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=150, max_depth=5), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyTies\", \"Title\",]],\n",
    "    [linear_model.LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilyTies\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Again, I did some parameter sweeps and testing to find the number of folds, estimators, and max_depths that \n",
    "# would return higher scores. 150 was a good number for estimators, and 4 folds was a better number than 3, 5, and 6.\n",
    "\n",
    "# Initialize the cross validation folds (originally 3, changed to 4 folds)\n",
    "kf = KFold(titanic.shape[0], n_folds=4, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print \"accuracy: \", accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing a Gradient Boosting Classifier, the score went down again! I'm quickly learning that adding more complexity to the model certainly doesn't mean that it is better! But then I started playing with parameter tuning within the script, and saw that changing some of the parameters actually increased my score back up to 83.95!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also read about a way that could pick out the best catagory from the list of predictors and visualize which ones made the biggest impact. In that example, they had added a bunch of new features and catagories that I thought wouldn't be impactful, so I'm going to run this on my model with less predictors than originally, with the logged age and the familyTies instead of sibsp and parch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAErCAYAAAAVANJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e2d7f5e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyTies\", \"Title\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the largest catagories that, according to this model, mattered where sex, pclass, and title, followed by fare. I would have expected age to have made a larger difference. Perhaps since I used the log of ages, it now matters less for predicting the model, even if the accuracy went up after changing it. Not surprisingly, the familyTies catagory had very little effect on the model, and may be worth dropping altogether. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.818264358265\n"
     ]
    }
   ],
   "source": [
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected for only the top 4 catagories, and got a score of 81.8, which went down again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.833944478785\n"
     ]
    }
   ],
   "source": [
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=300, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "\n",
    "print\"Score:\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still a believer in using Age, as my intuition tells me it is very important (as well as looking through the data). I plugged it back in, and got a much bette score, up to 83.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I think I'm ready to sumbit! \n",
    "##### I'm curious to how all of the testing, new features, and parameter tuning will do to my actual Kaggle score. I'm going to put together all of the tests that improved my score in theory, and then submit to kaggle to see if it was true!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.840628507295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keenan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:22: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=150, max_depth=5), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Title\"]],\n",
    "    [linear_model.LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "kf = KFold(titanic.shape[0], n_folds=4, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    for alg, predictors in algorithms:\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print \"accuracy: \", accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combined several features, using the gradient boosting classifier, selecting only the catagories that I knew were important, keeping the log of the ages, and parameter tuning for folds, estimators, and max depth, and got a score of about 84%!Now I'm going to generate a kaggle submission, and see how it compares! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Title'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-646-630383da1a2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictors\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Survived\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mfull_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/keenan/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1964\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1965\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/keenan/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2005\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2007\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2008\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/keenan/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Title'] not in index\""
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Title\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=150, max_depth=5), predictors],\n",
    "    [linear_model.LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "    \n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now getting an error which I cannot figure out. It won't let me us the Title feature when submitting. I tried submitting without the title class, and got a score that was lower than before. This is pretty annoying, but I guess a more complex model doesn't mean that its better. Anyways, I learned a lot during this project and hope to improve my score on a later date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
