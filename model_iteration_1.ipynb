{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 Iteration for Kaggle Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing all of the libraries that I need! I then read in the training dataset and see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn import *\n",
    "\n",
    "# We can use the pandas library in python to read in the csv file.\n",
    "# This creates a pandas dataframe and assigns it to the titanic variable.\n",
    "titanic = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "# Print the first 5 rows of the dataframe.\n",
    "print(titanic.head(5))\n",
    "print titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all ages were filled in, so I instead filled them in with the median value of all ages. I then changed all of the values of sex from 'male' or 'female' to numbers that I could acutally use in my calculations, being 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I also made the three different values of the port from which was embarked into tangible numbers as well, from S C or Q to 0 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == 'Q', \"Embarked\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used the LinearRegression function from sklearn on the training data, which I split into 3 different arrays, so I could test it against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  8.99877810e-02,   9.60756206e-01,   5.92676278e-01,\n",
      "         9.31138728e-01,   5.29343071e-02,   1.70275685e-01,\n",
      "         3.69943590e-01,   1.03474847e-01,   5.21597906e-01,\n",
      "         8.74491050e-01,   6.48883611e-01,   8.29742769e-01,\n",
      "         1.34797198e-01,  -1.61126844e-01,   6.58141307e-01,\n",
      "         6.39819748e-01,   1.51733875e-01,   2.95432718e-01,\n",
      "         5.35377959e-01,   6.21007683e-01,   2.61872592e-01,\n",
      "         2.62687561e-01,   7.31739160e-01,   5.05995897e-01,\n",
      "         5.61398567e-01,   3.35039734e-01,   1.30338808e-01,\n",
      "         4.68765767e-01,   6.60737753e-01,   9.10819218e-02,\n",
      "         4.77223920e-01,   1.04220026e+00,   6.60691613e-01,\n",
      "         8.71539273e-02,   5.28550732e-01,   4.01874338e-01,\n",
      "         1.30340307e-01,   1.29339672e-01,   5.72717129e-01,\n",
      "         6.65238822e-01,   4.83215779e-01,   7.60807408e-01,\n",
      "         1.30578363e-01,   8.71867121e-01,   7.09855487e-01,\n",
      "         9.11369897e-02,   1.39181745e-01,   6.60691613e-01,\n",
      "         6.82833485e-02,   6.06254374e-01,   4.92254383e-02,\n",
      "         1.29250392e-01,   9.02668258e-01,   7.51677954e-01,\n",
      "         3.19636822e-01,   5.05995897e-01,   8.23411477e-01,\n",
      "         1.27611544e-01,   8.16516947e-01,  -3.70209060e-02,\n",
      "         1.63085464e-01,   9.57981340e-01,   3.96742103e-01,\n",
      "         6.16138409e-02,   5.42714233e-01,   6.62112275e-02,\n",
      "         7.79751268e-01,   1.40293401e-01,   4.40592742e-01,\n",
      "         3.50534388e-02,   2.72709814e-01,   4.26360339e-01,\n",
      "         3.55241143e-01,   1.10226880e-01,   8.66078358e-02,\n",
      "         1.07366720e-01,   9.10819218e-02,   9.11369897e-02,\n",
      "         3.82661024e-01,   5.72471068e-01,   1.24221410e-01,\n",
      "         8.61972872e-02,   6.60705005e-01,   5.10138486e-01,\n",
      "         8.45241581e-01,   4.56477760e-01,   3.22699204e-02,\n",
      "         9.11369897e-02,   9.37604538e-01,   1.12967094e-01,\n",
      "         8.56794636e-02,   1.34727274e-01,   3.83320807e-01,\n",
      "         6.14970393e-03,  -7.83320148e-02,   9.11369897e-02,\n",
      "         3.10516665e-01,   5.49345421e-01,   7.23544338e-01,\n",
      "         2.33721448e-01,   5.81750798e-01,   9.10819218e-02,\n",
      "         5.25738424e-01,   6.40651310e-02,  -2.52427240e-02,\n",
      "         9.10819218e-02,   6.19865700e-01,   9.10387818e-02,\n",
      "         3.65066610e-02,   6.32939707e-01,   4.08195377e-01,\n",
      "         6.63657306e-01,   1.23882146e-01,   5.92491292e-01,\n",
      "         6.83623624e-01,   1.29295032e-01,  -6.19221217e-02,\n",
      "         2.59223480e-01,   6.09655955e-01,   5.30794378e-01,\n",
      "         2.88023805e-01,   9.11369897e-02,   2.82857942e-01,\n",
      "         7.61542726e-01,   3.45640063e-01,   1.85484998e-01,\n",
      "         1.70022737e-01,   1.12642722e-01,   5.59420117e-01,\n",
      "        -2.02485747e-03,   1.03290733e-01,   1.34440079e-01,\n",
      "         4.46807623e-01,   7.51677954e-01,   3.11805296e-01,\n",
      "         3.62947385e-01,   9.75724449e-01,   4.29554800e-01,\n",
      "         1.57043954e-01,   5.82928575e-01,   5.57105476e-01,\n",
      "         6.14443886e-01,   5.72812834e-01,   2.18783352e-01,\n",
      "         3.49472299e-01,   2.86040080e-01,   9.65037360e-02,\n",
      "         5.60916106e-01,   1.86919710e-01,   2.19027353e-01,\n",
      "         1.69739986e-01,   1.00690768e+00,  -5.89449777e-02,\n",
      "        -4.15452572e-02,   9.08736139e-02,   3.95827915e-01,\n",
      "         7.26175962e-01,   8.02219375e-02,   9.13557255e-02,\n",
      "        -2.22536096e-01,  -2.66919104e-02,   7.21593360e-01,\n",
      "         1.01953834e-01,   1.51388512e-01,   8.19705948e-02,\n",
      "         1.32518461e-01,   9.70245311e-01,   3.28974893e-01,\n",
      "         5.02576476e-01,   1.08437940e-01,   3.25183297e-01,\n",
      "         1.40818823e-01,   6.63268211e-01,   1.29295032e-01,\n",
      "         3.90965934e-01,   7.86503606e-02,  -3.68524682e-02,\n",
      "         9.13671691e-01,   2.84517666e-01,   4.46019673e-02,\n",
      "         2.68132779e-01,   3.35661255e-01,   1.96299597e-03,\n",
      "         3.51470400e-01,   6.51010647e-01,   5.11174133e-01,\n",
      "         6.29850621e-01,   4.10021732e-01,   4.03081359e-02,\n",
      "         4.74217131e-02,   7.64271489e-01,   3.44550453e-01,\n",
      "         5.97245007e-01,   3.69521460e-01,   9.46062691e-01,\n",
      "         9.12083149e-01,   1.70022737e-01,  -1.85251802e-02,\n",
      "         6.60691613e-01,   8.07931698e-01,   9.16548133e-02,\n",
      "        -2.22536096e-01,   5.78367977e-02,   3.48321010e-02,\n",
      "         1.45712251e-01,   6.91179799e-01,   3.84837497e-02,\n",
      "         1.45383056e-01,   7.26181926e-01,   4.78394987e-01,\n",
      "         1.12609974e-01,   7.50755869e-01,   1.23596450e-01,\n",
      "         2.84517666e-01,   1.36414068e-01,   1.01395495e+00,\n",
      "         5.87218752e-01,   1.90418359e-01,   1.02889863e+00,\n",
      "         2.83624866e-01,   1.56627303e-01,   3.00890244e-01,\n",
      "        -3.43861103e-02,   9.10819218e-02,   4.37274991e-01,\n",
      "         1.24346402e-01,   3.43657653e-01,   1.31782740e-01,\n",
      "         3.50007979e-01,   4.53816408e-01,   9.41986239e-01,\n",
      "         8.55812557e-02,   1.26427969e-01,   5.14461976e-01,\n",
      "         3.16370023e-01,   5.81627306e-01,   1.79146187e-01,\n",
      "         8.33217359e-01,   3.43657653e-01,   2.67886176e-01,\n",
      "         5.89980704e-01,   6.29850621e-01,   2.89082393e-01,\n",
      "         1.23551810e-01,   1.19423755e-01,   4.49914049e-01,\n",
      "         5.98080236e-01,   7.41700785e-01,   3.95976588e-01,\n",
      "         1.24570927e-01,   9.08512939e-02,   5.10217925e-01,\n",
      "         3.17243789e-01,   4.94880818e-02,   4.48434902e-01,\n",
      "         5.51647950e-01,   1.05176735e+00,   1.00396283e+00,\n",
      "         1.16824364e+00,   6.37295280e-01,   1.70022737e-01,\n",
      "         3.47081525e-02,   3.23790141e-01,   4.27827834e-01,\n",
      "         6.60691613e-01,   2.50879710e-01,   1.07703504e-04,\n",
      "         7.38026906e-02,   8.41682429e-01,   9.94221666e-01,\n",
      "         5.04388858e-01,   1.04634754e-01,   6.84091736e-01,\n",
      "         4.60920013e-01,   6.60691613e-01,   7.87205387e-01,\n",
      "         4.88920786e-01,   2.90790162e-01,   1.24446245e-01,\n",
      "         4.80968077e-01,  -3.19057282e-02,   9.10670657e-02,\n",
      "         1.57145126e-01,   1.40254724e-01,   5.02603260e-01,\n",
      "         1.03564537e-01,   8.07397611e-02,   1.23827078e-01,\n",
      "         2.19027353e-01,   6.93436769e-01,   1.02306096e+00,\n",
      "         1.07151871e+00,   2.91224311e-01,   6.03921666e-01,\n",
      "         1.12912026e-01,   5.42714233e-01,   1.54899175e-01]), array([ 1.13774791,  0.44173212,  0.98551347,  0.66915371,  0.08254228,\n",
      "        0.15142624,  0.83642014,  0.09704526,  0.64711481,  1.03845173,\n",
      "        1.06064212,  0.24647842,  0.98364902,  1.04411609,  1.10195734,\n",
      "        0.72596387,  0.09692709,  0.11388411,  0.60824987,  0.74905725,\n",
      "        0.090424  ,  1.00314273,  0.91588368,  0.13679886,  0.10365487,\n",
      "        0.82296458,  0.755174  , -0.27746285,  1.0035964 , -0.12636043,\n",
      "        0.70865678,  0.52438799,  1.06900476,  0.58044138,  0.32246331,\n",
      "        0.45904751,  0.0848131 ,  0.96838383,  0.09692709,  0.4123739 ,\n",
      "        0.96908901, -0.01732698,  0.33119158,  0.38953146,  0.97455471,\n",
      "        0.26457991,  0.28476325,  0.21075768,  0.78939013,  0.68174567,\n",
      "        0.5508181 ,  0.21132238,  0.00332574,  0.1315846 ,  0.44518065,\n",
      "        0.16116388,  0.07440511,  0.13363265,  0.09815645,  0.98913539,\n",
      "        0.69520122,  0.66925272,  0.66925272, -0.05732283,  0.25605759,\n",
      "        0.51306171,  0.04918447,  0.12689844,  0.08297663,  0.74556032,\n",
      "        0.63153497,  0.66915371,  1.03349593,  0.46795359,  0.11283671,\n",
      "        0.15759527,  0.5998862 ,  0.6125967 ,  0.96615292,  0.63469796,\n",
      "        0.6051113 ,  0.18499302,  0.15738453,  1.03364995,  0.80043282,\n",
      "        0.07003835,  0.85871777,  0.09692709,  0.37822123,  0.03771546,\n",
      "        0.70865678,  0.17123866,  0.87293786,  0.38692632,  0.14394491,\n",
      "       -0.00364112,  1.02362819,  0.60920867,  0.13721713,  0.57461098,\n",
      "        0.1534423 ,  0.29630296,  0.76221079,  0.0229439 ,  0.11050082,\n",
      "        0.59310377,  0.05272741,  0.64923598,  0.18004866, -0.05792355,\n",
      "        0.37724772,  0.14392897,  0.44776777,  0.09692709,  0.17057126,\n",
      "        0.97573347,  0.2546175 , -0.01069499,  0.59494436,  0.67712284,\n",
      "        0.81048116,  0.25112435,  0.7091068 ,  0.13414671,  0.21833626,\n",
      "        0.09018337,  0.5398775 ,  0.11371054,  0.09643219,  0.72214613,\n",
      "        0.83299143,  0.1712546 ,  0.07013414,  0.43870508,  0.5508181 ,\n",
      "        0.62795723,  0.17034196,  0.26289071,  1.03283656,  0.54234647,\n",
      "        0.66429253,  0.2888594 ,  0.24248073,  0.59832765,  0.15197868,\n",
      "        0.06672256,  0.76247901,  0.09709316,  0.62328105,  0.85873908,\n",
      "        0.39833841,  0.68526385,  0.28026543,  0.15249025,  0.0558822 ,\n",
      "        0.46338875,  0.3322838 ,  0.09704526,  0.12741893,  0.18977726,\n",
      "        0.90570685,  0.61255203,  0.1712546 ,  0.3041495 ,  0.05667859,\n",
      "        0.32003504,  0.13002433,  0.09704526,  0.02900113,  0.2546175 ,\n",
      "        0.25032727,  0.17123545,  0.71385691,  0.09643219,  0.03023685,\n",
      "        0.67057269,  0.83394424,  0.63668087,  0.45820842,  0.18004866,\n",
      "        0.03925263,  0.13700639,  0.76347615, -0.01610677,  0.2546175 ,\n",
      "       -0.05096587,  0.36065035,  0.49526401,  0.44776777,  0.88783867,\n",
      "        0.27650531,  0.0835897 ,  0.17095571,  0.0558822 ,  0.14352664,\n",
      "        0.26008209,  0.20422092,  0.14413971,  0.13917582,  0.78823881,\n",
      "        0.10244795,  0.983009  ,  0.12376157,  0.17152021,  0.71624816,\n",
      "        0.66906113,  0.5355726 ,  1.06327957,  0.55601524,  0.71952689,\n",
      "        0.43870508,  0.10813802,  0.14762674,  0.16452683,  0.09704526,\n",
      "        0.38468169,  0.77378051,  0.12353167,  0.31660245,  0.72019649,\n",
      "        0.18382257,  0.6683239 ,  0.07001598,  0.97445504,  0.13729376,\n",
      "        0.13363265,  0.88062695,  0.13363587,  0.08715737,  0.61255203,\n",
      "        0.5883169 ,  0.0229439 ,  0.18684089,  0.88743056,  0.13363587,\n",
      "        0.14770832,  0.62385335,  0.58195819,  0.89464072,  0.32433284,\n",
      "        1.0215796 ,  0.10198815,  1.01250232,  0.89757009,  0.52011358,\n",
      "        0.50665802,  0.19733591,  0.33882963,  0.19608356,  0.78269614,\n",
      "        0.3024605 ,  0.01303333,  0.35740293,  0.59528255,  0.2812701 ,\n",
      "        0.1713153 ,  0.17399933,  0.63510029,  0.2099606 ,  0.79897366,\n",
      "        0.62993975,  0.84335812,  0.49799211,  0.1712546 ,  0.01619374,\n",
      "        0.26496308,  0.09704526,  0.59494436,  0.03570385,  0.1574771 ,\n",
      "        0.55964686,  0.13363587,  0.0699841 ,  0.03391958,  0.68692335,\n",
      "        0.38475832,  0.66915371,  0.17777861,  0.16253816,  0.72211234,\n",
      "        0.83479538,  0.58677963,  0.07003835,  0.735757  ,  0.90451305,\n",
      "        0.09962007,  0.43250553,  0.13477258,  1.02529894,  0.13828479,\n",
      "        0.24105043,  0.13741193,  0.09704526,  0.04924194,  0.80169436,\n",
      "       -0.03139561,  0.64987806]), array([  1.72889219e-01,   1.70294715e-02,   7.82616935e-01,\n",
      "        -8.34788848e-03,   1.47022266e-01,   3.10888595e-01,\n",
      "         7.28261340e-01,   1.01479914e-01,   4.24565622e-01,\n",
      "         1.57316587e-02,   4.37708069e-01,   1.44204264e-02,\n",
      "         9.07678482e-02,   4.33913871e-01,   8.26537251e-01,\n",
      "         8.45262338e-01,   5.42776171e-01,   1.01763663e-01,\n",
      "         6.70148479e-01,   1.92163452e-01,   6.39359534e-02,\n",
      "         7.62650655e-01,   3.10124701e-02,   5.90024631e-01,\n",
      "         8.31356231e-01,   2.78648916e-01,   1.08309653e-01,\n",
      "         3.04531238e-01,   1.50864127e-01,   1.38986099e-01,\n",
      "         1.36219795e-01,   2.51197915e-01,   2.02625887e-01,\n",
      "         9.72357134e-01,   1.12191979e-01,   1.92169054e-01,\n",
      "         1.50211875e-01,  -2.14264992e-02,   4.52451020e-01,\n",
      "         4.38789988e-01,   6.04820088e-01,   7.89326541e-01,\n",
      "         8.00459867e-02,   2.10435721e-01,   5.70885269e-01,\n",
      "         5.70841743e-02,   1.44342132e-01,   1.00451104e+00,\n",
      "         6.42312317e-01,   8.51755703e-02,   7.33373007e-01,\n",
      "         3.09602117e-01,   1.49684208e-01,   3.22228832e-01,\n",
      "         1.01595923e-01,   6.50604478e-01,   1.01479914e-01,\n",
      "         8.45026241e-01,   1.38791822e-01,   7.14365273e-01,\n",
      "         7.68287651e-01,   1.84938938e-01,   1.01479914e-01,\n",
      "         6.54218524e-01,   2.93878313e-01,   2.96413137e-01,\n",
      "         1.92833539e-01,   8.27498735e-02,   3.28441263e-01,\n",
      "         5.87658439e-02,   1.02674988e-01,   1.42090676e-01,\n",
      "         2.83166248e-01,   1.01520440e-01,   2.10876914e-02,\n",
      "         9.01930011e-01,   6.80182444e-01,   3.63633521e-01,\n",
      "         4.29834748e-02,   2.51030051e-01,   2.71459394e-01,\n",
      "         1.55080767e-01,   1.20174297e-01,   6.76615822e-01,\n",
      "         5.21604336e-01,   2.74876851e-01,   7.14261845e-01,\n",
      "         4.63722197e-01,   1.43882255e-01,  -3.38493769e-02,\n",
      "         5.08333972e-02,   2.88240761e-01,   4.71949096e-03,\n",
      "         1.48920991e-01,   1.55073789e-01,   9.65241409e-01,\n",
      "         3.61956120e-01,   8.01212426e-01,   8.51755703e-02,\n",
      "         1.63090365e-01,   2.58489938e-01,   1.38385623e-01,\n",
      "         1.57316587e-02,   7.14397446e-01,   2.98282232e-01,\n",
      "         2.65779163e-02,   9.41922468e-01,   3.92478820e-01,\n",
      "         7.25879907e-01,   2.08234335e-01,   7.05625434e-02,\n",
      "         2.03820545e-01,   6.98106244e-01,   3.54986591e-01,\n",
      "         9.42312534e-01,   1.08182230e-01,   1.01115214e+00,\n",
      "         4.29882986e-01,   2.72580965e-01,   9.55913060e-02,\n",
      "         1.38553363e-01,   1.49766670e-01,   8.76445205e-01,\n",
      "         7.95521275e-01,   1.89563479e-01,   7.47402760e-02,\n",
      "         9.05943831e-01,   1.19035222e-01,   2.34961953e-01,\n",
      "         1.49265429e-01,   3.84688624e-01,   1.44070963e-01,\n",
      "         6.51000458e-01,   7.14396037e-01,   2.37161612e-01,\n",
      "         5.98123216e-01,   8.84762775e-01,   2.34195832e-01,\n",
      "         2.71459394e-01,   2.93878313e-01,   2.93878313e-01,\n",
      "         9.60495497e-02,   4.82543535e-01,   2.74738708e-01,\n",
      "         1.01479914e-01,   1.01479914e-01,   4.28725578e-01,\n",
      "         3.27845711e-01,   8.83507841e-01,   7.85083053e-02,\n",
      "         8.54020195e-02,   1.53868294e-01,   1.25458500e-01,\n",
      "         7.78614476e-01,   4.27536886e-01,   1.76095354e-01,\n",
      "         8.78367308e-01,   2.23270579e-01,   7.41615725e-02,\n",
      "         1.28260077e-01,   6.34105869e-01,   3.76826088e-01,\n",
      "         1.01513462e-01,   3.21161697e-01,   6.92919862e-02,\n",
      "         9.05219168e-01,   9.92643346e-02,   3.21100762e-02,\n",
      "         1.89869119e-01,   8.47257439e-01,   1.65792833e-01,\n",
      "         7.70032759e-01,   4.70822280e-01,   7.01001762e-01,\n",
      "         1.45018183e-01,   7.98992141e-02,   1.22365867e-01,\n",
      "        -5.62678525e-03,   6.34840292e-01,   1.47022266e-01,\n",
      "         6.21554022e-01,   1.55089154e-01,   1.92163452e-01,\n",
      "         7.45360827e-01,   1.92167645e-01,   8.15272492e-01,\n",
      "         7.49589740e-01,   9.59168970e-01,   4.23369546e-01,\n",
      "         6.56067455e-02,   1.17831761e-01,   1.17764665e-01,\n",
      "         6.77402825e-01,   1.31033823e-01,   2.11184136e-01,\n",
      "         3.61128670e-01,   1.92163452e-01,   3.27009298e-01,\n",
      "         2.80865752e-01,   4.73809464e-01,   1.17548012e-01,\n",
      "         2.08181789e-01,   8.39842956e-01,   6.07376016e-01,\n",
      "         1.36308792e-01,   5.71394060e-01,   2.34961953e-01,\n",
      "         7.32664113e-01,   4.58929866e-01,   2.99802486e-01,\n",
      "         1.07144857e-01,   8.54523415e-02,   3.79873628e-01,\n",
      "         6.77309159e-01,   2.08181789e-01,   8.74780819e-01,\n",
      "         1.12194764e-01,   3.71105893e-02,   2.30444621e-01,\n",
      "         5.78112549e-01,   8.80381008e-02,   4.38789988e-01,\n",
      "         6.50478673e-01,   2.52145211e-01,   2.16244600e-02,\n",
      "         7.72356638e-02,   7.64956968e-01,   1.06578734e-01,\n",
      "         3.85229660e-01,   6.33022282e-01,   6.89918839e-02,\n",
      "         1.92431836e-01,   8.51755703e-02,   4.59963761e-01,\n",
      "         1.92163452e-01,   7.52074841e-01,   6.94810438e-01,\n",
      "         3.74543331e-01,   1.47020857e-01,   1.28274033e-01,\n",
      "         1.54904640e-01,   8.83372143e-01,   1.38714930e-01,\n",
      "         1.01428183e-01,   6.37514393e-02,   4.74143535e-01,\n",
      "         1.44318380e-01,   3.32209243e-01,   9.85223737e-01,\n",
      "         1.12472244e-01,   1.60139061e-01,   2.66114644e-02,\n",
      "        -2.41362640e-01,   1.09304997e-01,   2.65882719e-01,\n",
      "         9.34799595e-01,   6.65962224e-02,  -1.44857067e-01,\n",
      "         7.32175244e-01,   1.01756702e+00,   6.57625381e-01,\n",
      "         6.82274953e-01,   7.78507074e-01,   3.06694232e-01,\n",
      "         7.03120381e-01,   1.47020857e-01,  -5.35194672e-02,\n",
      "         2.63450207e-01,   8.45198988e-01,   2.80865752e-01,\n",
      "         2.88522280e-01,   7.14342083e-01,   7.98068552e-01,\n",
      "         4.05781543e-01,   1.00941736e-01,   1.92789366e-01,\n",
      "         1.12191979e-01,   8.05473642e-01,   4.10332423e-01,\n",
      "        -6.55145848e-04,   7.89310178e-01,   7.38879084e-01,\n",
      "         1.43673989e-01,   1.49684208e-01,   1.01479914e-01,\n",
      "         8.33962978e-01,   8.06527571e-01,   7.46997500e-02,\n",
      "         6.54965242e-01,   2.67936850e-01,   1.17831761e-01,\n",
      "         6.75775470e-01,   2.72454182e-01,   9.99158265e-01,\n",
      "         5.87835137e-01,   4.84754956e-01,   1.70739321e-01])]\n"
     ]
    }
   ],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "print predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then put the three arrays back into one, and made the predictions binary again, either 0 or 1 for survival, so I could test vs the actual predictions of survivial or not. I counted all the correct ones and divided by the total number to get an accuracy rating of ~78 percent, which isn't great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783389450056\n"
     ]
    }
   ],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "# print predictions\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "# print predictions, titanic[\"Survived\"]\n",
    "correct = 0.0\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "     if (predictions[i] == titanic[\"Survived\"][i]):\n",
    "        correct += 1\n",
    "        \n",
    "accuracy = correct / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a Logistic Regression function from sklearen to compute the accuracy across all of the three different folds that I had used before and averaged the results, giving me a similar accuracy percent of ~78.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = linear_model.LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then cleaned the test data with the same conversions to numeric catagories as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "     PassengerId  Pclass                                               Name  \\\n",
      "0            892       3                                   Kelly, Mr. James   \n",
      "1            893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                          Myles, Mr. Thomas Francis   \n",
      "3            895       3                                   Wirz, Mr. Albert   \n",
      "4            896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "5            897       3                         Svensson, Mr. Johan Cervin   \n",
      "6            898       3                               Connolly, Miss. Kate   \n",
      "7            899       2                       Caldwell, Mr. Albert Francis   \n",
      "8            900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
      "9            901       3                            Davies, Mr. John Samuel   \n",
      "10           902       3                                   Ilieff, Mr. Ylio   \n",
      "11           903       1                         Jones, Mr. Charles Cresson   \n",
      "12           904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
      "13           905       2                               Howard, Mr. Benjamin   \n",
      "14           906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
      "15           907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
      "16           908       2                                  Keane, Mr. Daniel   \n",
      "17           909       3                                  Assaf, Mr. Gerios   \n",
      "18           910       3                       Ilmakangas, Miss. Ida Livija   \n",
      "19           911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
      "20           912       1                             Rothschild, Mr. Martin   \n",
      "21           913       3                          Olsen, Master. Artur Karl   \n",
      "22           914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
      "23           915       1                    Williams, Mr. Richard Norris II   \n",
      "24           916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
      "25           917       3                            Robins, Mr. Alexander A   \n",
      "26           918       1                       Ostby, Miss. Helene Ragnhild   \n",
      "27           919       3                                  Daher, Mr. Shedid   \n",
      "28           920       1                            Brady, Mr. John Bertram   \n",
      "29           921       3                                  Samaan, Mr. Elias   \n",
      "..           ...     ...                                                ...   \n",
      "388         1280       3                               Canavan, Mr. Patrick   \n",
      "389         1281       3                        Palsson, Master. Paul Folke   \n",
      "390         1282       1                         Payne, Mr. Vivian Ponsonby   \n",
      "391         1283       1     Lines, Mrs. Ernest H (Elizabeth Lindsey James)   \n",
      "392         1284       3                      Abbott, Master. Eugene Joseph   \n",
      "393         1285       2                               Gilbert, Mr. William   \n",
      "394         1286       3                           Kink-Heilmann, Mr. Anton   \n",
      "395         1287       1     Smith, Mrs. Lucien Philip (Mary Eloise Hughes)   \n",
      "396         1288       3                               Colbert, Mr. Patrick   \n",
      "397         1289       1  Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...   \n",
      "398         1290       3                     Larsson-Rondberg, Mr. Edvard A   \n",
      "399         1291       3                           Conlon, Mr. Thomas Henry   \n",
      "400         1292       1                            Bonnell, Miss. Caroline   \n",
      "401         1293       2                                    Gale, Mr. Harry   \n",
      "402         1294       1                     Gibson, Miss. Dorothy Winifred   \n",
      "403         1295       1                             Carrau, Mr. Jose Pedro   \n",
      "404         1296       1                       Frauenthal, Mr. Isaac Gerald   \n",
      "405         1297       2       Nourney, Mr. Alfred (Baron von Drachstedt\")\"   \n",
      "406         1298       2                          Ware, Mr. William Jeffery   \n",
      "407         1299       1                         Widener, Mr. George Dunton   \n",
      "408         1300       3                    Riordan, Miss. Johanna Hannah\"\"   \n",
      "409         1301       3                          Peacock, Miss. Treasteall   \n",
      "410         1302       3                             Naughton, Miss. Hannah   \n",
      "411         1303       1    Minahan, Mrs. William Edward (Lillian E Thorpe)   \n",
      "412         1304       3                     Henriksson, Miss. Jenny Lovisa   \n",
      "413         1305       3                                 Spector, Mr. Woolf   \n",
      "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                                Ware, Mr. Frederick   \n",
      "417         1309       3                           Peter, Master. Michael J   \n",
      "\n",
      "    Sex   Age  SibSp  Parch              Ticket      Fare            Cabin  \\\n",
      "0     0  34.5      0      0              330911    7.8292              NaN   \n",
      "1     1  47.0      1      0              363272    7.0000              NaN   \n",
      "2     0  62.0      0      0              240276    9.6875              NaN   \n",
      "3     0  27.0      0      0              315154    8.6625              NaN   \n",
      "4     1  22.0      1      1             3101298   12.2875              NaN   \n",
      "5     0  14.0      0      0                7538    9.2250              NaN   \n",
      "6     1  30.0      0      0              330972    7.6292              NaN   \n",
      "7     0  26.0      1      1              248738   29.0000              NaN   \n",
      "8     1  18.0      0      0                2657    7.2292              NaN   \n",
      "9     0  21.0      2      0           A/4 48871   24.1500              NaN   \n",
      "10    0  28.0      0      0              349220    7.8958              NaN   \n",
      "11    0  46.0      0      0                 694   26.0000              NaN   \n",
      "12    1  23.0      1      0               21228   82.2667              B45   \n",
      "13    0  63.0      1      0               24065   26.0000              NaN   \n",
      "14    1  47.0      1      0         W.E.P. 5734   61.1750              E31   \n",
      "15    1  24.0      1      0       SC/PARIS 2167   27.7208              NaN   \n",
      "16    0  35.0      0      0              233734   12.3500              NaN   \n",
      "17    0  21.0      0      0                2692    7.2250              NaN   \n",
      "18    1  27.0      1      0    STON/O2. 3101270    7.9250              NaN   \n",
      "19    1  45.0      0      0                2696    7.2250              NaN   \n",
      "20    0  55.0      1      0            PC 17603   59.4000              NaN   \n",
      "21    0   9.0      0      1             C 17368    3.1708              NaN   \n",
      "22    1  28.0      0      0            PC 17598   31.6833              NaN   \n",
      "23    0  21.0      0      1            PC 17597   61.3792              NaN   \n",
      "24    1  48.0      1      3            PC 17608  262.3750  B57 B59 B63 B66   \n",
      "25    0  50.0      1      0           A/5. 3337   14.5000              NaN   \n",
      "26    1  22.0      0      1              113509   61.9792              B36   \n",
      "27    0  22.5      0      0                2698    7.2250              NaN   \n",
      "28    0  41.0      0      0              113054   30.5000              A21   \n",
      "29    0  28.0      2      0                2662   21.6792              NaN   \n",
      "..   ..   ...    ...    ...                 ...       ...              ...   \n",
      "388   0  21.0      0      0              364858    7.7500              NaN   \n",
      "389   0   6.0      3      1              349909   21.0750              NaN   \n",
      "390   0  23.0      0      0               12749   93.5000              B24   \n",
      "391   1  51.0      0      1            PC 17592   39.4000              D28   \n",
      "392   0  13.0      0      2           C.A. 2673   20.2500              NaN   \n",
      "393   0  47.0      0      0          C.A. 30769   10.5000              NaN   \n",
      "394   0  29.0      3      1              315153   22.0250              NaN   \n",
      "395   1  18.0      1      0               13695   60.0000              C31   \n",
      "396   0  24.0      0      0              371109    7.2500              NaN   \n",
      "397   1  48.0      1      1               13567   79.2000              B41   \n",
      "398   0  22.0      0      0              347065    7.7750              NaN   \n",
      "399   0  31.0      0      0               21332    7.7333              NaN   \n",
      "400   1  30.0      0      0               36928  164.8667               C7   \n",
      "401   0  38.0      1      0               28664   21.0000              NaN   \n",
      "402   1  22.0      0      1              112378   59.4000              NaN   \n",
      "403   0  17.0      0      0              113059   47.1000              NaN   \n",
      "404   0  43.0      1      0               17765   27.7208              D40   \n",
      "405   0  20.0      0      0       SC/PARIS 2166   13.8625              D38   \n",
      "406   0  23.0      1      0               28666   10.5000              NaN   \n",
      "407   0  50.0      1      1              113503  211.5000              C80   \n",
      "408   1  28.0      0      0              334915    7.7208              NaN   \n",
      "409   1   3.0      1      1  SOTON/O.Q. 3101315   13.7750              NaN   \n",
      "410   1  28.0      0      0              365237    7.7500              NaN   \n",
      "411   1  37.0      1      0               19928   90.0000              C78   \n",
      "412   1  28.0      0      0              347086    7.7750              NaN   \n",
      "413   0  28.0      0      0           A.5. 3236    8.0500              NaN   \n",
      "414   1  39.0      0      0            PC 17758  108.9000             C105   \n",
      "415   0  38.5      0      0  SOTON/O.Q. 3101262    7.2500              NaN   \n",
      "416   0  28.0      0      0              359309    8.0500              NaN   \n",
      "417   0  28.0      1      1                2668   22.3583              NaN   \n",
      "\n",
      "    Embarked  \n",
      "0          2  \n",
      "1          0  \n",
      "2          2  \n",
      "3          0  \n",
      "4          0  \n",
      "5          0  \n",
      "6          2  \n",
      "7          0  \n",
      "8          1  \n",
      "9          0  \n",
      "10         0  \n",
      "11         0  \n",
      "12         0  \n",
      "13         0  \n",
      "14         0  \n",
      "15         1  \n",
      "16         2  \n",
      "17         1  \n",
      "18         0  \n",
      "19         1  \n",
      "20         1  \n",
      "21         0  \n",
      "22         0  \n",
      "23         1  \n",
      "24         1  \n",
      "25         0  \n",
      "26         1  \n",
      "27         1  \n",
      "28         0  \n",
      "29         1  \n",
      "..       ...  \n",
      "388        2  \n",
      "389        0  \n",
      "390        0  \n",
      "391        0  \n",
      "392        0  \n",
      "393        0  \n",
      "394        0  \n",
      "395        0  \n",
      "396        2  \n",
      "397        1  \n",
      "398        0  \n",
      "399        2  \n",
      "400        0  \n",
      "401        0  \n",
      "402        1  \n",
      "403        0  \n",
      "404        1  \n",
      "405        1  \n",
      "406        0  \n",
      "407        1  \n",
      "408        2  \n",
      "409        0  \n",
      "410        2  \n",
      "411        2  \n",
      "412        0  \n",
      "413        0  \n",
      "414        1  \n",
      "415        0  \n",
      "416        0  \n",
      "417        1  \n",
      "\n",
      "[418 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "print(titanic_test[\"Sex\"].unique())\n",
    "\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'male', \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'female', \"Sex\"] = 1\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'Q', \"Embarked\"] = 2\n",
    "\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "\n",
    "print titanic_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then made predictions on the test set using the training algorithm from beforehand. Finally, I created a submission file for kaggle in the form of a csv for submission! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = linear_model.LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One Complete! Now on to making the model better\n",
    "_____\n",
    "\n",
    "I want to improve my model to get a better accuracy score than ~78 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
